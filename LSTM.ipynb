{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Lets look at a Recurrent Neural Network, which is a neural network that feeds a output to its input.  In essence, this looks like multiple versions of the same network chained together. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the picture above we can see that the output of Xo is fed into the next node, alongside the its input, X1.  By the time we arrive at Xt, we have persisted some learned information, throughout the network.  These networks are very good at predicting information, however can struggle with what is know as “long-term dependencies” which are essentially some logical meaning that is drawn from information learned a long time ago in the network.  This proves a problem for RNN when trying to predict information that relies upon long term dependencies.  This is were, Long Short Term Memory (LSTM) networks come in.  Long Short Term Memory networks are a special type of RNN</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The difference between a normal RNN network and a LSTM network is regular RNN's combine the node input and the chained input using something like a tan function</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Inside a LSTM cell we have a different structure - lets take it step by step\n",
    "</p>\n",
    "<p>One of the first things to notice about the LSTM cell is the top row.  This row allows the information fed along the cell to pass through with little difference in operation. It would seem that the extra information that is incorporated from the input section, Xt is multiplied into the copy of the Ht-1 data and then some other information is combined - using addition - later on (but we will come to that)\n",
    "</p>\n",
    "<p>The first layer of out LSTM cell is the \"forget gate layer\".  This is usede to decide what information to forget - funnily enough.  This is done by taking the output of the previous node, Ht-1 and the input of this node, Xt and performs a sigmoid function on it.  \n",
    "</p>\n",
    "<p>The sigmoid is a mathematical function, looking like 'S' -shaped curve. The output of the sigmoid function is between 0 and 1.  In our context, the closer to 0, the less we let through; the closer to 1, the more we let through.  Think of it as 0% to 100%</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This can be expressed as either on of the following notation\n",
    "$$ S(x) =  \\frac{\\mathrm{e^x} }{\\mathrm{1} + e^x }  $$\n",
    "<br>   \n",
    "$$ S(x) =  \\frac{1 }{\\mathrm{1} + e^-x }  $$ </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>With a the sigmoid function looking like this</p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5866175789173301"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have the sigmoid translated into python\n",
    "import math\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "sigmoid(0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Lucklily we have a handy Tensorflow function that will perform \n",
    "the same operation:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Sigmoid_3:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "i = tf.sigmoid(0.35)\n",
    "print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Below we have the forget gate layer with each of its elements, as well as the mathematical notation for the function ft</p><img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\" width=\"500\" height=\"500\">\n",
    "<p>The function can be understood as the simoid activation function of the dot product of the node Weight and the Ht-1 value and the Xt value - with finally adding the bias of the function</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3_Python_3",
   "language": "python",
   "name": "jupyter3_python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
